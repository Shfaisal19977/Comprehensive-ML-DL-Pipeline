{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkcaIp1z+SJDPzkWPIExbe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"62xiz-v3NwPH"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# RNN Model with Attention Class\n","class RNNWithAttention:\n","    def __init__(self, input_dim, output_dim, units, n_classes):\n","        self.model = tf.keras.Sequential([\n","            layers.Embedding(input_dim=input_dim, output_dim=output_dim),\n","            layers.Bidirectional(layers.LSTM(units, return_sequences=True)),\n","            layers.Attention(),\n","            layers.Dense(n_classes, activation='softmax')\n","        ])\n","        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n","        return self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n","\n","    def evaluate(self, X_test, y_test):\n","        return self.model.evaluate(X_test, y_test)\n","\n","# Usage Example Function\n","def run_rnn_attention(X, y, input_dim, output_dim=128, units=128, n_classes=10, epochs=10):\n","    X_train, X_test, y_train, y_test = prepare_data(X, y)\n","\n","    rnn_attention_model = RNNWithAttention(input_dim=input_dim, output_dim=output_dim, units=units, n_classes=n_classes)\n","    rnn_attention_model.train(X_train, y_train, X_test, y_test, epochs=epochs)\n","\n","    print(\"Evaluation Results:\", rnn_attention_model.evaluate(X_test, y_test))\n","\n","# Sample usage with custom dataset\n","# run_rnn_attention(X, y, input_dim=10000, n_classes=10, epochs=20)\n"]}]}